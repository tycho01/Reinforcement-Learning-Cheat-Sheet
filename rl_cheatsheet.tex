\documentclass[10pt,landscape]{article}
\usepackage{multicol}
\usepackage{xcolor}
\usepackage{calc}
\usepackage{ifthen}
\usepackage[landscape]{geometry}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{algorithm2e}
\usepackage{float}

% To make this come out properly in landscape mode, do one of the following
% 1.
%  pdflatex rl_cheatsheet.tex
%
% 2.
%  latex rl_cheatsheet.tex
%  dvips -P pdf  -t landscape rl_cheatsheet.dvi
%  ps2pdf rl_cheatsheet.ps


% If you're reading this, be prepared for confusion.  Making this was
% a learning experience for me, and it shows.  Much of the placement
% was hacked in; if you make it better, let me know...


% 2008-04
% Changed page margin code to use the geometry package. Also added code for
% conditional page margins, depending on paper size. Thanks to Uwe Ziegenhagen
% for the suggestions.

% 2006-08
% Made changes based on suggestions from Gene Cooperman. <gene at ccs.neu.edu>


% To Do:
% \listoffigures \listoftables
% \setcounter{secnumdepth}{0}


% This sets page margins to .5 inch if using letter paper, and to 1cm
% if using A4 paper. (This probably isn't strictly necessary.)
% If using another size paper, use default 1cm margins.
\ifthenelse{\lengthtest { \paperwidth = 11in}}
        { \geometry{top=.5in,left=.5in,right=.5in,bottom=.5in} }
        {\ifthenelse{ \lengthtest{ \paperwidth = 297mm}}
                {\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
                {\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
        }

% Turn off header and footer
\pagestyle{empty}


% Redefine section commands to use less space
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%x
                                {\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
                                {-1explus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%
                                {\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {1ex plus .2ex}%
                                {\normalfont\small\bfseries}}
\makeatother

% Define BibTeX command
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% Don't print section numbers
\setcounter{secnumdepth}{0}


\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}

\DeclareMathOperator*{\argmax}{argmax}

% -----------------------------------------------------------------------

\begin{document}

\newcommand\pro{\item[$+$]}
\newcommand\con{\item[$-$]}

\raggedright
\footnotesize
\begin{multicols}{3}

% multicol parameters
% These lengths are set only within the two main columns
%\setlength{\columnseprule}{0.25pt}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{2pt}

\begin{center}
    \Large{Reinforcement Learning Cheat Sheet} \\
\end{center}

\section{Terminology}

\begin{description}
    % https://en.wikipedia.org/wiki/List_of_mathematical_symbols
    % https://en.wikipedia.org/wiki/List_of_mathematical_symbols_by_subject
    % https://en.wikipedia.org/wiki/Notation_in_probability_and_statistics

    % \item[$$] 
    % \item[$lowercase$]
    % scalar / part
    % \item[$uppercase$]
    % space / total / function / matrix
    % \item[$bold$]
    % vector
    % \item[$blackboard bold$]
    % set
    \item[$a$]
    action, arm (multi-armed bandit problem)
    \item[$A$]
    advantage
    \item[$\alpha$]
    step size
    \item[$b$]
    bandit, baseline estimate, bias
    \item[$c$]
    cost, context (= state)
    \item[$d$]
    difference error
    \item[$\Delta$]
    difference
    \item[$D$]
    replay buffer
    % \item[$\partial$]
    % partial derivative
    \item[$E$]
    eligibility trace % error
    % \item[$\mathbb{E} / E$]
    % expected value
    \item[$\epsilon$]
    learning rate (0~1)
    % \item[$f$]
    % function
    \item[$\phi$]
    state transition function
    \item[$g$]
    gain (reward over time, target)
    \item[$\gamma$]
    discount factor
    \item[$\nabla$]
    gradient (spacial derivative)
    \item[$H$]
    horizon
    % \item[$i$]
    % this round
    % \item[$j$]
    % next round
    \item[$J$]
    expected return (see loss)
    % \item[$k$]
    % some amount
    \item[$L$]
    expected (squared) loss (see return)
    \item[$\lambda$]
    trace decay (0~1)
    \item[$m$]
    momentum % amount, matrix rows
    % \item[$\mu$]
    % mean
    % \item[$n$]
    % count
    % \item[$\mathbb{N}$]
    % natural number space
    % \item[$O / \Omega$]
    % order of magnitude
    % \item[$p$]
    % probability (0~1)
    \item[$Pr$]
    transition distribution
    \item[$\pi$]
    policy ($state \rightarrow action$)
    % \item[$\Pi$]
    % product
    \item[$q$]
    quality
    \item[$r$]
    reward
    \item[$R$]
    return
    \item[$\rho$]
    regret
    % \item[$\mathbb{R}$]
    % real number space
    \item[$s$]
    state
    % \item[$\sigma$]
    % standard deviation
    % \item[$\Sigma$]
    % sum
    % \item[$t$]
    % time (sub), target value (given)
    \item[$\tau$]
    trajectory
    \item[$\theta$]
    parameter weight (learning target) %, small positive number (iteration)
    \item[$\theta_{i}^{-}$]
    target network
    % \item[$u$]
    % user
    % \item[$U$]
    % ??? (DQN)
    \item[$v$]
    value
    \item[$vm$]
    variance momentum
    \item[$w$]
    weight, winning probability
    % \item[$x$]
    % observation
    % \item[$y$]
    % prediction, output
    \item[$_{*}$]
    optimal
    \item[$'$]
    next / derivative
    \item[$\hat{}$]
    estimation
    \item[$‖$]
    vector norm
\end{description}

\section{Agent-Environment Interface}
\includegraphics[width=\linewidth]{./images/Agent-Environment.png}
The Agent at each step $t$ receives a representation of the environment's \emph{state},
$S_t \in S$ and it selects an action $A_t \in A(s)$.
Then, as a consequence of its action the agent receives a \emph{reward}, $R_{t + 1} \in R \in \mathbb{R}$.

\section{Policy}
A \emph{policy} is a mapping from a state to an action
\begin{equation}
    \pi_t(s|a)
    % \label{eq: policy}
\end{equation}
That is the probability of select an action $A_t = a$ if $S_t = s$.

% \newlength{\MyLen}
% \settowidth{\MyLen}{\texttt{letterpaper}/\texttt{a4paper} \ }

\section{Reward}
The total \emph{reward} is expressed as:
\begin{equation}
    G_t = \sum_{k = 0}^H \gamma^kr_{t + k + 1}
    % \label{eq: total_reward}
\end{equation}
Where $\gamma$ is the \emph{discount factor} and $H$ is the \emph{horizon}, that can be infinite.

\section{Bandits}
A class of decision methods modeling how to pick an action in the absence of observable
state influencing the outcome, signified by the multi-armed bandit problem.
This reduces reinforcement learning to the sub-problem of exploration vs exploitation.

They estimate $\hat{r}_{a} $$\approx$$ \mathbb E[r|a]$

\subsection{Greedy}
\begin{equation}
    n_{a} = \sum_{t:a_{t}=a} 1 ; \hat{r}_{a} = \sum_{t:a_{t}=a} \frac{r_{t}}{n_{a}}; a_{t} = \argmax\limits_{a \in A} \hat{r}_{a}
\end{equation}

\subsection{Optimistic-Greedy}
large initial $\hat{r}_{a}$, R

\subsection{\texorpdfstring{$\epsilon$}{Epsilon}-Greedy}
with probability $\epsilon$, pick randomly

\subsection{Upper Confidence Bound}
try all for k rounds, then
\begin{equation}
    a_{t} = \argmax\limits_{a \in A} { \hat{r}_{a} + \sqrt{\frac{2 log t}{n_{a}}} }
\end{equation}

\subsection{Contextual Bandit (LinUCB)}

\begin{equation}
    x_{t,a} = \Phi(s_{t,a}); \mathbb E[r_{t,a}|x_{t,a}] = \theta_{a} \cdot x_{t,a}; \hat{\theta}_{a} = A^{-1} b
\end{equation}

\subsection{Posterior / Thompson sampling}
choose by probability actions maximize expected reward

\subsection{Greedy in the Limit with Infinite Exploration (GLIE)}
infinitely explores, converges on greedy

\section{Markov Decision Process}

A \textbf{Markov Decision Process}, MPD, is a 5-tuple $(S, A, P, R, \gamma)$ where:
\begin{equation}
        \begin{array}{l}
         \text{finite set of states:} \\
                s \in S \\
         \text{finite set of actions:} \\
        a \in A \\
                 \text{state transition probabilities:} \\
        p(s' | s, a) = Pr \{S_{t + 1} = s' | S_t = s, A_t = a \} \\
                \text{expected reward for state-action-nexstate:}\\
        r(s',s, a) = \mathbb{E}[ R_{t + 1} | S_{t + 1} = s',  S_t = s, A_t = a ]  \\
        \end{array}
\end{equation}

\section{Value Function}

Value function describes \emph{how good} it is to be in a specific state $s$ under a certain policy $\pi$. For MDP:

\begin{equation}
    V_{\pi}(s) = \mathbb{E}[G_t | S_t = s]
    \label{eq: value_func}
\end{equation}

Informally, it is the expected return (expected cumulative discounted reward) when starting from $s$ and following $\pi$.

\subsection{Optimal}

\begin{equation}
    v_*(s) = \max\limits_{\pi} v^\pi(s)
    \label{eq: value_optimal}
\end{equation}

\section{Action-Value (Q) Function}

We can also denote the expected reward for state-action pairs.

\begin{equation}
    q_{\pi}(s, a) = \mathbb{E}_{\pi}\begin{bmatrix} G_t | S_t = s, A_t = a \end{bmatrix}
    \label{eq: q_func}
\end{equation}

\subsection{Optimal}

The optimal value-action function is denoted as:

\begin{equation}
    q_*(s,a) = \max\limits_{\pi} q^\pi(s,a)
    \label{eq: action_value_optimal}
\end{equation}

Clearly, using this new notation we can redefine $v^*$,
equation \ref{eq: value_optimal}, using $q^*(s,a)$,
equation \ref{eq: action_value_optimal}:

\begin{equation}
    v_*(s) = \max\limits_{a \in A(s)} q_{\pi*}(s,a)
\end{equation}

Intuitively, the above equation expresses the fact that the value of a state under the optimal policy \textbf{must be equal} to the expected return from the best action from that state.

\section{model-based methods (known MDP)}

\section{Exhaustive Search}

\section{Bellman Equation}
An important recursive property emerges for both Value (\ref{eq: value_func})
and Q (\ref{eq: q_func}) functions if we expand them.

\section{Value Function}
\begin{equation}
\begin{array}{l l}
v_{\pi}(s) &= \mathbb{E}_{\pi}\begin{bmatrix}G_t | S_t = s\end{bmatrix} \\
\\
&=  \mathbb{E}_{\pi}\begin{bmatrix}\sum\limits_{k = 0}^{\infty} \gamma^kR_{t + k + 1} | S_t = s \end{bmatrix} \\
\\
&= \mathbb{E}_{\pi}\begin{bmatrix}R_{t + 1} + \sum\limits_{k = 0}^{\infty} \gamma^k R_{t + k + 2} | S_t = s \end{bmatrix} \\
\\
&= \underbrace{\sum\limits_{a} \pi(a | s) \sum\limits_{s'}\sum\limits_{r} p(s', r | s, a)}_{\text{Sum of all probabilities $\forall$ possible $r$}} \\
& \begin{bmatrix} r + \gamma \underbrace{\mathbb{E}_{\pi}\begin{bmatrix} \sum\limits_{k = 0}^{\infty} \gamma^k R_{t + k + 2} | S_{t + 1} = s' \end{bmatrix}}_{\text{Expected reward from } s_{t + 1}} \end{bmatrix} \\
\\
&= \sum\limits_{a} \pi(a | s) \sum\limits_{s'}\sum\limits_{r} p(s', r | s, a)\begin{bmatrix} r + \gamma v_{\pi}(s')
\end{bmatrix}
\end{array}
\label{eq: value_bellman}
\end{equation}

Similarly, we can do the same for the Q function:

\begin{equation}
\begin{array}{l l}
q_{\pi}(s,a) &= \mathbb{E}_{\pi}\begin{bmatrix} G_t | S_t = s, A_t = a \end{bmatrix} \\
\\
&= \mathbb{E}_{\pi}\begin{bmatrix} \sum\limits_{k = 0}^{\infty}\gamma^kR_{t + k + 1} | S_t = s, A_t = a \end{bmatrix} \\
\\
&= \mathbb{E}_{\pi}\begin{bmatrix}R_{t+1} + \sum\limits_{k = 0}^{\infty}\gamma^kR_{t + k + 2} | S_t = s, A_t = a \end{bmatrix} \\
\\
&=\sum\limits_{s',r} p(s',r|s,a)\begin{bmatrix}r +\mathbb{E}_{\pi} \begin{bmatrix} \sum\limits_{k = 0}^{\infty} \gamma^k R_{t + k + 2} | S_{t+1} = s' \end{bmatrix} \end{bmatrix} \\
\\
&=\sum\limits_{s',r} p(s',r|s,a)\begin{bmatrix}r +\gamma V_{\pi}(s') \end{bmatrix}\\
\end{array}
\label{eq: action_value_bellman}
\end{equation}

\section{Contraction Mapping}

Let $(X, d)$ be a metric space and $f: X \rightarrow X$. We say that $f$ is a
\emph{contraction} if there is a \emph{Lipschitz coefficient} $k \in [0, 1)$
such that for all $x$ and $y$ in $X$:

\begin{equation*}
    d(f(x), f(y)) \leq k d(x, y)
\end{equation*}

\subsection{Contraction Mapping theorem}

For complete metric space $(X,d)$ and contraction $f: X \rightarrow X$,
there is only one fixed point $x^\ast$ such that

\begin{equation*}
    f(x^\ast) = x^\ast
\end{equation*}

Moreover, if $x$ is any point in $X$ and $f^n(x)$ is inductively defined by
$f^2(x) = f(f(x))$, $f^3(x)=f(f^2(x))$, \ldots, $f^n(x)=f(f^{n−1}(x))$,
then $f^n(x) \rightarrow x^\ast$ as $n \rightarrow \infty$. This theorem guarantees
a unique optimal solution for the dynamic programming algorithms detailed below.

\section{Dynamic Programming}

Taking advantages of the subproblem structure of the V and Q
function we can find the optimal policy by just \emph{planning}.

\subsection{Policy Iteration}

We can now find the optimal policy:

\begin{algorithm}[H]
%\SetAlgoLined
 1. Initialisation \\
 $V(s) \in \mathbb{R}, (\text{e.g } V(s) = 0)$ and $\pi(s) \in A$  for all $s \in S$,\\
 $\Delta \leftarrow 0$ \\
 2. Policy Evaluation \\
 \While{$\Delta < \theta$ (a small positive number)}{
  \ForEach{$s \in S$} {
        $v \leftarrow V(s)$\\
        $V(s) \leftarrow \sum\limits_a \pi(a|s) \sum\limits_{s', r} p(s',r | s, a) \begin{bmatrix}
                r + \gamma V(s')
        \end{bmatrix}$ \\
        $\Delta \leftarrow \max(\Delta, | v - V(s)|)$
        }
 }
 3. Policy Improvement \\
 \emph{policy-stable} $ \leftarrow $ \emph{true} \\
 \While{not policy-stable}{
  \ForEach{$s \in S$} {
        \emph{old-action} $\leftarrow \pi(s)$
        $\pi(s) \leftarrow \argmax\limits_a\sum\limits_{s', r} p(s',r | s, a)  \begin{bmatrix}
            r + \gamma V(s')
        \end{bmatrix}$ \\
        \emph{policy-stable} $\leftarrow$ \emph{old-action} $ \neq \pi(s)$
  }
 }
% \caption{Policy Iteration}
\end{algorithm}

Policy iteration methods:
\begin{itemize}
\item gradient-based (policy gradient methods): gradient ascent
\item gradient-free: simulated annealing, cross-entropy search or methods of evolutionary computation
\item value search/iteration: stop after 1 state sweep
\item async DP: update iteratively, no full sweeps
\item generated policy iteration (GPI)
\end{itemize}

% \end{multicols}

% \begin{multicols}{3}

\subsection{Value Iteration}
We can avoid to wait until $V(s)$ has converged and instead do policy improvement and truncated policy evaluation step in one operation
\begin{algorithm}[H]
  \SetKwInOut{Output}{ouput}
 Initialise $V(s) \in \mathbb{R}, \text{e.g} V(s) = 0$ \\
 $\Delta \leftarrow 0$ \\
 \While{$\Delta < \theta$ (a small positive number)}{
  \ForEach{$s \in S$} {
        $v \leftarrow V(s)$\\
        $V(s) \leftarrow \max\limits_a \sum\limits_{s', r} p(s',r | s, a) \begin{bmatrix}
                r + \gamma V(s')
        \end{bmatrix}$ \\
        $\Delta \leftarrow \max(\Delta, | v - V(s)|)$
  }
 }
 \Output{Deterministic policy $\pi \approx \pi_*$ such that}
 $\pi(s) = \argmax\limits_a \sum\limits_{s', r} p(s',r | s, a) \begin{bmatrix}
     r + \gamma V(s')
  \end{bmatrix}$
\end{algorithm}

\section{Model-free methods}

\section{Monte Carlo (MC) Methods}
episodic, based on \textbf{averaging sample returns} for each state-action pair.
Implementation:

\begin{algorithm}[H]
 Initialise for all $s \in S, a \in A(s):$ \\
        $\quad Q(s,a) \leftarrow \text{arbitrary}$ \\
        $\quad \pi(s) \leftarrow \text{arbitrary}$ \\
        $\quad Returns(s,a) \leftarrow \text{empty list}$ \\
 \While{forever}{
    Choose $S_0 \in S$ and $A_0 \in A(S_0)$, all pairs have probability $ > 0$ \\
    Generate an episode starting at $S_0, A_0$ following $\pi$
    \ForEach{pair $s,a$ appearing in the episode}{
        $G \leftarrow$ return following the first occurrence of $s,a$ \\
        Append $G$ to $Returns(s,a))$ \\
        $Q(s,a) \leftarrow average(Returns(s,a))$ \\
    }
    \ForEach{$s$ in the episode}{
        $\pi(s) \leftarrow \argmax\limits_a Q(s,a)$
    }
 }
\end{algorithm}

For non-stationary problems, the Monte Carlo estimate for, e.g, $V$ is:
\begin{equation}
    V(S_t) \leftarrow V(S_t) + \alpha \begin{bmatrix} G_t - V(S_t) \end{bmatrix}
\end{equation}
Where $\alpha$ is the learning rate, how much we want to forget about past experiences.

\section{Temporal Difference (TD)}
combines DP's bootstrap (learn mid-episode) w\/ MC's sampling.
substitutes expected discounted reward $G_t$ from the episode with an estimation:
\begin{equation}
    V(S_t) \leftarrow V(S_t) + \alpha \begin{bmatrix} R_{t + 1} + \gamma V(S_{t+1} - V(S_t) \end{bmatrix}
\end{equation}
See Sarsa below for a sample implementation.

\subsection{Sarsa}
Sarsa (State-action-reward-state-action) is a on-policy TD control.
Can use priors.
Update rule:

\begin{equation*}
    Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[r_t + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \right]
\end{equation*}

\begin{algorithm}[H]
    Initialise $Q(s,a)$ arbitrarily and $Q(terminal-state, ·) = 0$\\
    \ForEach{episode $\in$ episodes}{
        Choose $a$ from $s$ using policy derived from $Q$ (e.g., $\epsilon$-greedy) \\
        \While{$s$ is not terminal} {
            Take action $a$, observer $r, s'$ \\
            Choose $a'$ from $s'$ using policy derived from $Q$ (e.g., $\epsilon$-greedy) \\
            $Q(s,a) \leftarrow Q(s,a) + \alpha  \begin{bmatrix} r +  \gamma Q(s',a') - Q(s,a) \end{bmatrix}$\\
            $s \leftarrow s'$ \\
            $a \leftarrow a'$
        }
    }
\end{algorithm}
   
\subsection{\texorpdfstring{$n$}{n}-step Sarsa (\texorpdfstring{$n$}{n}-step TD)}

Define the $n$-step Q-Return

\begin{equation*}
    q_{t}^{(n)} = R_{t+1} + \gamma R_{t+2} + \ldots + \gamma^{n-1} R_{t+n} + \gamma^n Q(S_{t+n})
\end{equation*}

$n$-step Sarsa update $Q(s, a)$ towards the $n$-step Q-return

\begin{equation*}
    Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[q_t^{(n)} - Q(s_t, a_t) \right]
\end{equation*}

\subsection{Forward View Sarsa(\texorpdfstring{$\lambda$}{lambda}) (々 TD(\texorpdfstring{$\lambda$}{lambda}))}

\begin{equation*}
    q_t^\lambda = (1-\lambda) \sum_{n=1}^\infty \lambda^{n-1} q_t^{(n)}
\end{equation*}

\begin{equation*}
    Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[q_t^\lambda - Q(s_t, a_t) \right]
\end{equation*}

\subsection{Backward View Sarsa(\texorpdfstring{$\lambda$}{lambda}) (々 TD(\texorpdfstring{$\lambda$}{lambda}))}

\begin{itemize}
    \pro more efficient
    \pro can update at every time-step
    \item eligibility traces (per $s/a$ pair): find cause in frequency vs recency
\end{itemize}

\begin{equation*}
    E_0(s,a) = 0
\end{equation*}

\begin{equation*}
    E_0(s,a) = \gamma \lambda E_{t-1}(s,a) + \mathbb{1}(S_t=t,A_t=a)
\end{equation*}

\begin{equation*}
    \delta_t = R_{t+1} + \gamma Q(S_{t+1},A_{t+1}) - Q(S_t,A_t)
\end{equation*}

\begin{equation*}
    Q(s,a) \Leftarrow Q(s,a) + \alpha \delta_t E_t(s,a)
\end{equation*}

\section{Linear Function Approximation}
general + efficient learning

update temporal difference error, minimize squared loss:

\begin{equation*}
    \delta \leftarrow r_{t} + \gamma\theta^{T}\phi(s_{t+1}) - \theta^{T}\phi(s_{t})
    \theta \leftarrow \theta + \alpha\delta\phi(s_{t})
    J(\theta) = ||\delta||^{2}
\end{equation*}

\section{Q Learning}

\begin{equation*}
    \delta \leftarrow r_{t} + \gamma \argmax\limits_{a \in A} Q(\phi(s_{t+1},a); \theta_{i}^{-}) - Q(\phi(s_{t},a); \theta_{i})
\end{equation*}

\begin{equation*}
    J(\theta) = ||\delta||^{2}
\end{equation*}

Update rule:

\begin{equation*}
    Q(s,a) \leftarrow Q(s,a) + \alpha  \begin{bmatrix}r +  \gamma \max\limits_{a'}Q(s',a') - Q(s,a)\end{bmatrix}
\end{equation*}

Replay Memory (update $\theta$ w\/ SGD):

\begin{equation*}
    \nabla_{\theta_{i}}J(\theta_{i}) = \mathbb{E}_{s_{t},a_{t},r_{t},s_{t+1} \sim D} \delta \nabla_{\theta_{i}}Q(\phi(s_{t},a_{t}); \theta)
\end{equation*}

\subsection{Deep Q Learning}

Created by $DeepMind$, Deep Q Learning, DQL, substitutes the $Q$
function with a deep neural network called \emph{Q-network}.
It keeps some observation in a $memory$ to train the network on.

\begin{equation*}
    Y_{DQN} = r_t + \gamma \max\limits_{a \in A} Q(\Phi(s_{t+1},a); \theta_i^{-})
\end{equation*}

\begin{equation}
\nabla_{i}(\theta_{i}) = \mathbb{E}_{(s, a, r, s') \sim U(D)}
\begin{bmatrix}
        ( \underbrace{r + \gamma \max\limits_{a} Q(s',a'; \theta_{i-1})}_\text{target} - \\
        \underbrace{Q(s,a;\theta_i)}_\text{prediction})^2
\end{bmatrix}
\end{equation}

Where $\theta$ are the weights of the network and $U(D)$ is the experience replay history.

\begin{algorithm}[H]
 Initialise replay memory $D$ with capacity $N$\\
 Initialise $Q(s,a)$ arbitrarily \\
 \ForEach{episode $\in$ episodes}{
        \While{$s$ is not terminal} {
         With probability $\epsilon$ select a random action $a \in A(s)$ \\
         otherwise select $a = \max\limits_{a} Q(s, a; \theta)$ \\
         Take action $a$, observer $r, s'$ \\
         Store transition $(s, a, r, s')$ in $D$ \\
         Sample random minibatch of transitions $(s_j, a_j, r_j, s'_j)$ from $D$ \\
         $\text{Set } y_i \leftarrow
           \begin{cases}
               r_j & \text{for terminal } s_j'\\
               r_j + \gamma \max\limits_a Q(s',a'; \theta) & \text{for non-terminal } s'_j
           \end{cases}$ \\
         Perform gradient descent step on $(y_j - Q(s_j, a_j;\Theta))^2 $ \\
         $s \leftarrow s'$
        }
 }
% \caption{Deep Q Learning}
\end{algorithm}

\subsection{Prioritized Replay Memory}
learn esp. from high loss (traumas)

\begin{equation*}
    p(s_{t},a_{t},r_{t},s_{t+1}) \propto r_{t} + \gamma \max\limits_{a \in A} Q(\Phi(s_{t+1},a); \theta_{i}^{-})
\end{equation*}

\subsection{Double DQN}
solve bias from reused $\theta$, faster

\begin{equation*}
    Y_{DQN} = r_t + \gamma Q(\Phi(s_{t+1}, \argmax\limits_{a \in A} Q(\Phi(s_{t+1},a); \textcolor{red}{\theta_i})); \textcolor{red}{\theta_i^{-}})
\end{equation*}

\section{Policy Gradient}
learn $\pi$.
\begin{itemize}
    \pro simpler than $Q$ or $V$
    \pro allows stochastic policy (rock-paper-scissors)
    \con local optimum
    \con less sample efficient
\end{itemize}

\begin{equation*}
    L = \mu (log p(a|s) \cdot R(s))
\end{equation*}

\section{Likelihood Ratio}

return state-action trajectory:
\begin{equation*}
    R(\tau) = \sum_{t=0}^T R(s_t,a_t)
\end{equation*}

expected return:
\begin{equation*}
\begin{array}{l l}
    J(\theta) &= \mathbb{E}\begin{bmatrix} \sum_{t=0}^T R(s_t,a_t); \pi_\theta \end{bmatrix} \\
    \\
    &= \sum_\tau^T R(s_t,a_t); \pi_\theta \\
\end{array}
\end{equation*}

find $\theta$ to max:
\begin{equation*}
    \max\limits_\theta J(\theta) = \max\limits_\theta \sum_\tau P(\tau; \theta) R(\tau)
\end{equation*}

\section{REINFORCE}

\begin{itemize}
    \item Uses a policy during an episode to collect information on states, actions and rewards.
    \item Computes the return for each episode using the rewards collected.
    \item Updates the model parameters in the direction of the policy gradient.
\end{itemize}

\begin{equation*}
    \nabla_\theta J(\theta)=\sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_{t}|s_{t}) R
\end{equation*}

desired loss function:
\begin{equation*}
    \frac{1}{m}\sum_1^m \nabla_\theta \log \pi_\theta(a_t|s_t) R
\end{equation*}

\subsection{Baselined REINFORCE}
use baselined rewards

\begin{equation*}
    \nabla_\theta J(\theta)=\sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_{t}|s_{t}) (R - V_{\phi(s_{t})})
\end{equation*}

\section{Actor-critic}
\begin{itemize}
    \pro less variance than Baselined REINFORCE
    \item actor (makes policy): policy gradient
    \item critic: policy iteration
\end{itemize}

advantage:
\begin{equation}
    A_\pi(s,a) = Q(s,a) - V(s)
    % \label{eq: advantage}
\end{equation}

\subsection{Asynchronous Advantage Actor Critic (A3C)}
\begin{itemize}
    \item 5-step Q-Value estimation
    \item shares params between actor/critic
    \pro run parallel, one policy
    \pro no more need for DQN's replay policy
\end{itemize}

\begin{equation*}
    Q(s_t,a_t) = E[r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \dots + \gamma^n V(s_{t+n})]
\end{equation*}

%---------------------------------------------------------------------------

\rule{0.3\linewidth}{0.25pt} \\
Copyright \copyright\ 2018 Francesco Saverio Zuppichini

\scriptsize

\href{https://github.com/FrancescoSaverioZuppichini/Reinforcement-Learning-Cheat-Sheet}{https://github.com/FrancescoSaverioZuppichini/Reinforcement-Learning-Cheat-Sheet}

\end{multicols}
\end{document}
